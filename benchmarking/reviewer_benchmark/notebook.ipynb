{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing health check on http://localhost:11434\n",
      "Ollama is running\n",
      "Health check passed\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "import json\n",
    "from pydantic.json import pydantic_encoder\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../backend/.env')\n",
    "sys.path.append(\"../../\")\n",
    "from backend.LLM.AnythingLLM_client import AnythingLLMClient\n",
    "from backend.Reviewer import Reviewer\n",
    "from backend.LLM.OllamaLLM import OllamaAI\n",
    "from backend.database.schemas import DraftResult\n",
    "\n",
    "\n",
    "anyllm_client = AnythingLLMClient(\"http://localhost:3001/api\", \"3WMNAPZ-GYH4RBE-M67SR00-7Y7KYEF\")\n",
    "ollama_client = OllamaAI('http://localhost:11434', 'llama3:instruct')\n",
    "reviewer = Reviewer(ollama_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def review_draft(email:str, response_email:str, sources:List[str]):\n",
    "    base_prompt = f\"\"\"\n",
    "    From this enquiry:\n",
    "    Enquiry: {email}\n",
    "    ---\n",
    "    Does the response answer the enquiry?\n",
    "    Response: {response_email}\n",
    "    ---\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time_hallucination = time.time()\n",
    "    draft_score = reviewer.hallucination_eval(base_prompt, sources)\n",
    "    end_time_hallucination = time.time()\n",
    "    \n",
    "    start_time_linkert = time.time()\n",
    "    linkert_score = reviewer.linkert_eval(base_prompt)\n",
    "    end_time_linkert = time.time()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"linkert_score\": linkert_score,\n",
    "        \"hallucination_score\": draft_score,\n",
    "        \"linkert_time\": end_time_linkert - start_time_linkert,\n",
    "        \"hallucination_time\": end_time_hallucination - start_time_hallucination\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already evaluated results_fake_email_10.json\n",
      "Already evaluated results_fake_email_undergrad_18.json\n",
      "Already evaluated results_fake_email_undergrad_3.json\n",
      "Already evaluated results_fake_email_3.json\n",
      "Already evaluated results_fake_email_undergrad_14.json\n",
      "Already evaluated results_fake_email_undergrad_15.json\n",
      "Already evaluated results_fake_email_2.json\n",
      "Already evaluated results_fake_email_undergrad_2.json\n",
      "Already evaluated results_fake_email_undergrad_19.json\n",
      "Already evaluated results_fake_email_11.json\n",
      "Already evaluated results_fake_email_undergrad_12.json\n",
      "Already evaluated results_fake_email_undergrad_9.json\n",
      "Already evaluated results_fake_email_9.json\n",
      "Already evaluated results_fake_email_16.json\n",
      "Already evaluated results_fake_email_undergrad_5.json\n",
      "Already evaluated results_fake_email_5.json\n",
      "Already evaluated results_fake_email_4.json\n",
      "Already evaluated results_fake_email_undergrad_4.json\n",
      "Already evaluated results_fake_email_17.json\n",
      "Already evaluated results_fake_email_8.json\n",
      "Already evaluated results_fake_email_undergrad_8.json\n",
      "Already evaluated results_fake_email_undergrad_13.json\n",
      "Already evaluated results_fake_email_undergrad_10.json\n",
      "Already evaluated results_fake_email_18.json\n",
      "Already evaluated results_fake_email_7.json\n",
      "Already evaluated results_fake_email_undergrad_7.json\n",
      "Already evaluated results_fake_email_14.json\n",
      "Already evaluated results_fake_email_15.json\n",
      "Already evaluated results_fake_email_undergrad_6.json\n",
      "Already evaluated results_fake_email_6.json\n",
      "Already evaluated results_fake_email_19.json\n",
      "Already evaluated results_fake_email_undergrad_11.json\n",
      "Already evaluated results_fake_email_1.json\n",
      "Already evaluated results_fake_email_undergrad_1.json\n",
      "Already evaluated results_fake_email_12.json\n",
      "Already evaluated results_fake_email_undergrad_16.json\n",
      "Already evaluated results_fake_email_undergrad_17.json\n",
      "Already evaluated results_fake_email_13.json\n",
      "Already evaluated results_fake_email_undergrad_0.json\n",
      "Reviewing results_fake_email_0.json\n"
     ]
    }
   ],
   "source": [
    "import json as simplejson\n",
    "drafts_generated_path = \"../one_vs_multi_shots/results\"\n",
    "path_to_emails = \"../../dataset\"\n",
    "\n",
    "already_evaluated = os.listdir(\"results\")\n",
    "    \n",
    "for draft_file in os.listdir(drafts_generated_path):\n",
    "    \n",
    "    if f\"reviewer_{draft_file}\" in already_evaluated:\n",
    "        print(f\"Already evaluated {draft_file}\")\n",
    "        continue\n",
    "    \n",
    "    with open(f\"{drafts_generated_path}/{draft_file}\", 'r') as f:\n",
    "        draft_generated_data = json.load(f)\n",
    "        single_response_email = draft_generated_data['single_shot']['response']['textResponse']\n",
    "        single_sources = draft_generated_data['single_shot']['response']['sources']\n",
    "        multi_response_email = draft_generated_data['multi_step']['response']['generated_draft_email']\n",
    "        multi_sources = [simplejson.loads(generated_answer['attributes']['sources']) for generated_answer in draft_generated_data['multi_step']['response']['generated_answers']]\n",
    "        multi_sources = [source for sources in multi_sources for source in sources]\n",
    "\n",
    "    enquiry_path = f\"{path_to_emails}/{draft_file.replace('results_', '')}\"\n",
    "    with open(enquiry_path, 'r') as f:\n",
    "        email_data = json.load(f)\n",
    "        email = email_data['email']\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"Reviewing {draft_file}\")\n",
    "    results_path = f\"results/reviewer_{draft_file}\"\n",
    "    multi_step_result = review_draft(email, multi_response_email, multi_sources)\n",
    "    single_shot_result = review_draft(email, single_response_email, single_sources)\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"single_shot\": single_shot_result,\n",
    "            \"multi_step\": multi_step_result\n",
    "        }, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_evaluated = os.listdir(\"results\")\n",
    "markdowns_path = \"markdowns\"\n",
    "os.makedirs(markdowns_path, exist_ok=True)\n",
    "\n",
    "for draft_file in os.listdir(drafts_generated_path):\n",
    "    filename = f\"reviewer_results_{draft_file}\"\n",
    "    if f\"{filename}\" not in already_evaluated:\n",
    "        print(f\"not evaluated {filename}\")\n",
    "        continue\n",
    "    \n",
    "    with open(f\"{drafts_generated_path}/{draft_file}\", 'r') as f:\n",
    "        draft_generated_data = json.load(f)\n",
    "        response_email = draft_generated_data[method]['response']['textResponse']\n",
    "        sources = draft_generated_data[method]['response']['sources']    \n",
    "        \n",
    "    enquiry_path = f\"{path_to_emails}/{draft_file.replace('results_', '')}\"\n",
    "    with open(enquiry_path, 'r') as f:\n",
    "        email_data = json.load(f)\n",
    "        email = email_data['email']\n",
    "    \n",
    "    with open(f\"{markdowns_path}/human_reviewer_{draft_file.replace(\".json\", \".md\")}\", 'w') as f:\n",
    "        # email and response in the markdown \n",
    "        text = f\"\"\"\n",
    "# Enquiry {draft_file}\n",
    "\n",
    "{email}\n",
    "\n",
    "# Response\n",
    "\n",
    "{response_email}\n",
    "\n",
    "        \"\"\"\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time linkert single: 9.356014943122863\n",
      "Average time linkert multi: 10.173141956329346\n",
      "Average time hallucination single: 14.548383140563965\n",
      "Average time hallucination multi: 16.558142638206483\n",
      "Average time linkert: 9.764578449726105\n",
      "Average time hallucination: 15.553262889385223\n"
     ]
    }
   ],
   "source": [
    "average_time_linkert_single = []\n",
    "average_time_linkert_multi = []\n",
    "average_time_hallucination_single = []\n",
    "average_time_hallucination_multi = []\n",
    "\n",
    "for result_file in os.listdir(\"results\"):\n",
    "    with open(f\"results/{result_file}\", 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        average_time_linkert_single.append(data['single_shot']['linkert_time'])\n",
    "        average_time_hallucination_single.append(data['single_shot']['hallucination_time'])\n",
    "        average_time_linkert_multi.append(data['multi_step']['linkert_time'])\n",
    "        average_time_hallucination_multi.append(data['multi_step']['hallucination_time'])\n",
    "\n",
    "avg_linkert_single = sum(average_time_linkert_single) / len(average_time_linkert_single)\n",
    "avg_linkert_multi = sum(average_time_linkert_multi) / len(average_time_linkert_multi)\n",
    "avg_hallucination_single = sum(average_time_hallucination_single) / len(average_time_hallucination_single)\n",
    "avg_hallucination_multi = sum(average_time_hallucination_multi) / len(average_time_hallucination_multi)\n",
    "\n",
    "\n",
    "print(f\"Average time linkert single: {avg_linkert_single}\")\n",
    "print(f\"Average time linkert multi: {avg_linkert_multi}\")\n",
    "print(f\"Average time hallucination single: {avg_hallucination_single}\")\n",
    "print(f\"Average time hallucination multi: {avg_hallucination_multi}\")\n",
    "\n",
    "avg_linkert = (avg_linkert_single + avg_linkert_multi) / 2\n",
    "avg_hallucination = (avg_hallucination_single + avg_hallucination_multi) / 2\n",
    "\n",
    "print(f\"Average time linkert: {avg_linkert}\")\n",
    "print(f\"Average time hallucination: {avg_hallucination}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
