{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "%pip install rouge -q\n",
    "%pip install sacrebleu -q\n",
    "\n",
    "# Load the questions\n",
    "dataset = \"../../answer_questions/results/benchmark_results_responses_undergrad.json\"\n",
    "\n",
    "with open(dataset, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "results = data['results']\n",
    "\n",
    "generated_vs_gold = []\n",
    "for file in results:\n",
    "    for question_pos in results[file]:\n",
    "        generated_vs_gold.append({\n",
    "            'question': question_pos['question'],\n",
    "            'gold': question_pos['gold_answer'],\n",
    "            'generated': question_pos['generated_answer']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1 Score: 0.4578\n",
      "Average ROUGE-2 F1 Score: 0.2958\n",
      "Average ROUGE-L F1 Score: 0.4261\n",
      "Average BLEU Score: 23.1957\n"
     ]
    }
   ],
   "source": [
    "# Rouge metric\n",
    "\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge_scores = []\n",
    "for q in generated_vs_gold:\n",
    "    scores = rouge.get_scores(q['generated'], q['gold'])\n",
    "    rouge_scores.append({\n",
    "        'question': q['question'],\n",
    "        'rouge-1': scores[0]['rouge-1'],\n",
    "        'rouge-2': scores[0]['rouge-2'],\n",
    "        'rouge-l': scores[0]['rouge-l'],\n",
    "    })\n",
    "    \n",
    "bleu_scores = []\n",
    "for q in generated_vs_gold:\n",
    "    bleu = sacrebleu.raw_corpus_bleu([q['generated']], [[q['gold']]])\n",
    "    bleu_scores.append({\n",
    "        'question': q['question'],\n",
    "        'bleu': bleu.score\n",
    "    })\n",
    "    \n",
    "rouge_df = pd.DataFrame(rouge_scores)\n",
    "bleu_df = pd.DataFrame(bleu_scores)\n",
    "\n",
    "average_rouge_1 = rouge_df['rouge-1'].apply(lambda x: x['f']).mean()\n",
    "average_rouge_2 = rouge_df['rouge-2'].apply(lambda x: x['f']).mean()\n",
    "average_rouge_l = rouge_df['rouge-l'].apply(lambda x: x['f']).mean()\n",
    "\n",
    "average_bleu = bleu_df['bleu'].mean()\n",
    "\n",
    "# Display the average scores\n",
    "print(\"Average ROUGE-1 F1 Score: {:.4f}\".format(average_rouge_1))\n",
    "print(\"Average ROUGE-2 F1 Score: {:.4f}\".format(average_rouge_2))\n",
    "print(\"Average ROUGE-L F1 Score: {:.4f}\".format(average_rouge_l))\n",
    "print(\"Average BLEU Score: {:.4f}\".format(average_bleu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{lin-2004-rouge,\n",
    "    title = \"{ROUGE}: A Package for Automatic Evaluation of Summaries\",\n",
    "    author = \"Lin, Chin-Yew\",\n",
    "    booktitle = \"Text Summarization Branches Out\",\n",
    "    month = jul,\n",
    "    year = \"2004\",\n",
    "    address = \"Barcelona, Spain\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/W04-1013\",\n",
    "    pages = \"74--81\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{post-2018-call,\n",
    "  title = \"A Call for Clarity in Reporting {BLEU} Scores\",\n",
    "  author = \"Post, Matt\",\n",
    "  booktitle = \"Proceedings of the Third Conference on Machine Translation: Research Papers\",\n",
    "  month = oct,\n",
    "  year = \"2018\",\n",
    "  address = \"Belgium, Brussels\",\n",
    "  publisher = \"Association for Computational Linguistics\",\n",
    "  url = \"https://www.aclweb.org/anthology/W18-6319\",\n",
    "  pages = \"186--191\",\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
